[General]
outdir = ../models/custom_dual_encoder
model_name = trained_on_zelda_iterative
suffix =
model_path = ../models/custom_dual_encoder/trained_on_zelda_iterative

[Data]
corpus_name = zelda
label_type = nel
document_level = true
#downsample_factor = 0.1
verbalizations_path = zelda_labels_verbalizations.json
labels_path = None
use_corpus_labels = False
replace_labels = False
replace_verbalizations = False
verbalization_strategy = title+description+categories
max_verbalization_length_soft = 50

[Model]
greedy = true
insert_in_context = 0
insert_which_labels = gold_pred
label_embedding_batch_size = 128
transformer = bert-base-uncased
pooling = first_last
context_size = 0
negative_sampling_strategy = hard
negative_sampling_factor = 1
loss = cross_entropy
similarity_metric = euclidean
constant_updating = True

[Training Parameters]
epochs = 20
batch_size = 32
batch_size_eval = 32
learning_rate = 5e-6
seed = 123

